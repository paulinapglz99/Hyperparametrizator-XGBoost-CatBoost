{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df28470c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import cohen_kappa_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.optimize import minimize\n",
    "from functools import reduce\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8203b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define all types of targets\n",
    "\n",
    "ordinal_targets = ['ADNC', 'Braak', 'Thal', 'CERAD']\n",
    "continuous_targets = ['percent 6e10 positive area', 'percent AT8 positive area', \n",
    "                      'percent GFAP positive area', 'percent NeuN positive area']\n",
    "all_target_columns = ordinal_targets + continuous_targets\n",
    "\n",
    "### Categorical mappings for submission format\n",
    "MAPS = {\n",
    "    'ADNC': {0:\"Not AD\", 1:\"Low\", 2:\"Intermediate\", 3:\"High\"},\n",
    "    'Braak': {i:f\"Braak {['0','I','II','III','IV','V','VI'][i]}\" for i in range(7)},\n",
    "    'CERAD': {0:\"Absent\", 1:\"Sparse\", 2:\"Moderate\", 3:\"Frequent\"},\n",
    "    'Thal': {i:f\"Thal {i}\" for i in range(6)}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a7c87af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Reorder datasets\n",
    "\n",
    "# ### Read column order from CSV\n",
    "column_order = pd.read_csv('data/column_order.csv')['column_name'].tolist()\n",
    "\n",
    "### Load datasets\n",
    "df_mtg = pd.read_csv('data/dataset_mtg_dropped.csv')\n",
    "df_a9 = pd.read_csv('data/dataset_a9_dropped.csv')\n",
    "\n",
    "### Reorder datasets using the predefined column order\n",
    "df_mtg = df_mtg[column_order]\n",
    "df_a9 = df_a9[column_order]\n",
    "df_mtg.to_csv('data/false_dataset_a9_ordered.csv', index=False)\n",
    "df_a9.to_csv('data/false_dataset_mtg_ordered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "291b6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Test models\n",
    "\n",
    "def test_model(df, target, region):\n",
    "    \n",
    "    ## Load trained model from disk\n",
    "    model = joblib.load(f'./models/{region}/{region}_{target}_model.pkl')\n",
    "    \n",
    "    ## Prepare test data (drop other targets and create feature matrix)\n",
    "    other_targets = [t for t in all_target_columns if t != target]\n",
    "    X = df.drop(columns=['Donor ID', target] + other_targets)\n",
    "    y = df[target].values\n",
    "    donor_ids = df['Donor ID'].values\n",
    "    \n",
    "    ## Make predictions (continuous values from XGBoost regressor)\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    ## Calculate final metrics based on target type\n",
    "    \n",
    "    ## For ordinal targets (ADNC, Braak, Thal, CERAD) - Goal 1\n",
    "    if target in ordinal_targets:\n",
    "        \n",
    "        ## Load optimized thresholds from trained model\n",
    "        optimized_thresholds = model.optimized_thresholds_\n",
    "        \n",
    "        ## Apply thresholds to convert continuous predictions to ordinal classes\n",
    "        y_classes = np.digitize(y_pred, bins=optimized_thresholds)\n",
    "        \n",
    "        ## Calculate DREAM benchmark metrics for ordinal targets (QWK primary, MAE and Spearman secondary)\n",
    "        metrics = {\n",
    "            'quadratic_weighted_kappa': cohen_kappa_score(y, y_classes, weights='quadratic'),\n",
    "            'mean_absolute_error': mean_absolute_error(y, y_classes),\n",
    "            'spearman_correlation': spearmanr(y, y_classes)[0],\n",
    "            'thresholds_used': str(optimized_thresholds.round(3))\n",
    "        }\n",
    "        \n",
    "        ## Create predictions dataframe with ordinal classes\n",
    "        predictions_df = pd.DataFrame({'Donor ID': donor_ids, 'true_value': y, 'predicted_value': y_classes})\n",
    "    \n",
    "    else:\n",
    "        ## For continuous targets (6e10, AT8, GFAP, NeuN) - Goal 2\n",
    "        \n",
    "        ## Calculate concordance correlation coefficient (CCC) - primary DREAM metric\n",
    "        mean_t, mean_p = np.mean(y), np.mean(y_pred)\n",
    "        var_t, var_p = np.var(y, ddof=1), np.var(y_pred, ddof=1)\n",
    "        ccc = (2 * np.cov(y, y_pred)[0,1]) / (var_t + var_p + (mean_t - mean_p)**2)\n",
    "        \n",
    "        ## Calculate DREAM benchmark metrics for continuous targets (CCC primary, MSE and R2 secondary)\n",
    "        metrics = {\n",
    "            'concordance_correlation_coefficient': ccc,\n",
    "            'mean_squared_error': mean_squared_error(y, y_pred),\n",
    "            'r2_score': r2_score(y, y_pred)\n",
    "        }\n",
    "        \n",
    "        ## Create predictions dataframe with continuous values\n",
    "        predictions_df = pd.DataFrame({'Donor ID': donor_ids, 'true_value': y, 'predicted_value': y_pred})\n",
    "    \n",
    "    ## Save test predictions to CSV\n",
    "    os.makedirs(f'./output/{region}', exist_ok=True)\n",
    "    predictions_df.to_csv(f'./output/{region}/{region}_{target}_test_predictions.csv', index=False)\n",
    "    \n",
    "    ## Save test metrics to CSV\n",
    "    pd.DataFrame(list(metrics.items()), columns=['metric_name', 'metric_value']).to_csv(\n",
    "        f'./output/{region}/{region}_{target}_test_metrics.csv', index=False)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "### Process all test datasets with their respective trained models\n",
    "datasets = {'MTG': df_mtg, 'A9': df_a9}\n",
    "test_metrics = {region: {target: test_model(df, target, region) \n",
    "                         for target in all_target_columns} for region, df in datasets.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea549cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Format test submissions\n",
    "\n",
    "def format_evaluation(f, mode='train'):\n",
    "\n",
    "    ## Extract target name from filename (format: {region}_{target}_train_predictions.csv)\n",
    "    parts = f.split('/')[-1].replace(f'_{mode}_predictions.csv', '').split('_')\n",
    "    target = '_'.join(parts[1:])\n",
    "    \n",
    "    ## Load test predictions and ensure data type (string) in Donor ID\n",
    "    df = pd.read_csv(f)[['Donor ID', 'predicted_value']]\n",
    "    df['Donor ID'] = df['Donor ID'].astype(str)\n",
    "    \n",
    "    ## Convert predictions: categorical to strings, continuous to clipped float [0,100]\n",
    "    if target in MAPS:\n",
    "        df['predicted_value'] = df['predicted_value'].round().astype(int).map(MAPS[target])\n",
    "    else:\n",
    "        df['predicted_value'] = df['predicted_value'].clip(0, 100).astype(float)\n",
    "    \n",
    "    ## Format column name for submission\n",
    "    col_name = f\"predicted {target.replace('percent ','').replace(' positive area','')}\"\n",
    "    return df.rename(columns={'predicted_value': col_name})\n",
    "\n",
    "### Process all predictions per target and region\n",
    "for region in ['MTG', 'A9']:\n",
    "    files = [f'./output/{region}/{region}_{t}_test_predictions.csv' for t in all_target_columns]\n",
    "    reduce(lambda l,r: l.merge(r, on='Donor ID'), map(lambda f: format_evaluation(f, 'test'), files)).to_csv(\n",
    "        f'./output/{region}/submission_{region}_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc8c1969",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Compare models between regions\n",
    "\n",
    "def compare_target(target):\n",
    "    \n",
    "    ## Determine primary metric based on target type\n",
    "    metric_name = 'quadratic_weighted_kappa' if target in ordinal_targets else 'concordance_correlation_coefficient'\n",
    "    ## Extract metrics from both regions\n",
    "    mtg_metric = test_metrics['MTG'][target][metric_name]\n",
    "    a9_metric = test_metrics['A9'][target][metric_name]\n",
    "    \n",
    "    return {\n",
    "        'Target': target,\n",
    "        'Metric': 'QWK' if target in ordinal_targets else 'CCC',\n",
    "        'MTG_Score': mtg_metric,\n",
    "        'A9_Score': a9_metric,\n",
    "        'Best_Region': 'MTG' if mtg_metric > a9_metric else 'A9',\n",
    "        'Best_Score': max(mtg_metric, a9_metric),\n",
    "        'Difference': abs(mtg_metric - a9_metric)\n",
    "    }\n",
    "\n",
    "pd.DataFrame([compare_target(t) for t in all_target_columns]).to_csv('./output/model_comparison.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
