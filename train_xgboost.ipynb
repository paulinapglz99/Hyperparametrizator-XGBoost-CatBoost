{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d51c6b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import cohen_kappa_score, mean_absolute_error, mean_squared_error, r2_score\n",
    "from scipy.stats import spearmanr\n",
    "from scipy.optimize import minimize\n",
    "import os\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbc06c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define targets\n",
    "ordinal_targets = ['ADNC', 'Braak', 'Thal', 'CERAD']\n",
    "continuous_targets = ['percent 6e10 positive area', 'percent AT8 positive area', 'percent GFAP positive area', 'percent NeuN positive area']\n",
    "all_target_columns = ordinal_targets + continuous_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51b92ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load hyperparameters from CSVs for each target and region\n",
    "\n",
    "def load_hyperparams():\n",
    "    \n",
    "    ## Initialize dictionary to store hyperparameters by target\n",
    "    hyperparams = {}\n",
    "    \n",
    "    ## Loop through each brain region\n",
    "    for region in ['MTG', 'A9']:\n",
    "        \n",
    "        ## Read hyperparameters CSV and convert to nested dictionary structure\n",
    "        df_hp = pd.read_csv(f'hyperparam/{region}_hyperparameters.csv')\n",
    "        hyperparams[region] = df_hp.set_index('target').drop(columns=['best_score']).to_dict('index')\n",
    "    \n",
    "    return hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51faeb1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Train final model\n",
    "def train_final_model(df, target, region, hyperparams_dict, random_state=42):\n",
    "    \n",
    "    ## Prepare data (create feature matrix (x) without other targets and target vectors (y))\n",
    "    other_targets = [t for t in all_target_columns if t != target]\n",
    "    X = df.drop(columns=['Donor ID', target] + other_targets)\n",
    "    y = df[target].values\n",
    "    donor_ids = df['Donor ID'].values\n",
    "\n",
    "    ## Get hyperparameters for this target and region\n",
    "    params = hyperparams_dict[region][target]\n",
    "    \n",
    "    ## Train model (XGBoost regressor with optimized hyperparameters)\n",
    "    model = xgb.XGBRegressor(random_state=random_state, eval_metric='rmse', **params)\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    ## Calculate metrics based on target type (according to DREAM tasks)\n",
    "\n",
    "    ## For ordinal targets\n",
    "    if target in ordinal_targets:\n",
    "\n",
    "        ## Optimize thresholds\n",
    "        class_props = pd.Series(y).value_counts(normalize=True).sort_index()\n",
    "        initial_th = np.quantile(y_pred, q=np.cumsum(class_props).iloc[:-1])\n",
    "        \n",
    "        result = minimize(\n",
    "            lambda th: -cohen_kappa_score(y, np.digitize(y_pred, bins=np.sort(th)), weights='quadratic'),\n",
    "            x0=initial_th, method='Nelder-Mead'\n",
    "        )\n",
    "        \n",
    "        ## Store optimized thresholds as model attribute\n",
    "        optimized_thresholds = np.sort(result.x)\n",
    "        model.optimized_thresholds_ = optimized_thresholds\n",
    "\n",
    "        ## Apply thresholds to get ordinal predictions\n",
    "        y_classes = np.digitize(y_pred, bins=optimized_thresholds)\n",
    "\n",
    "        ## Calculate metrics for ordinal targets\n",
    "        metrics = {\n",
    "            'quadratic_weighted_kappa': cohen_kappa_score(y, y_classes, weights='quadratic'),\n",
    "            'mean_absolute_error': mean_absolute_error(y, y_classes),\n",
    "            'spearman_correlation': spearmanr(y, y_classes)[0],\n",
    "            'optimized_thresholds': str(optimized_thresholds.round(3))\n",
    "        }\n",
    "        \n",
    "        ## Calculate predictions for ordinal targets\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'Donor ID': donor_ids,\n",
    "            'true_value': y,\n",
    "            'predicted_value': y_classes\n",
    "        })\n",
    "\n",
    "    else:\n",
    "        \n",
    "        ## Calculate metrics for continuous targets\n",
    "        mean_t, mean_p = np.mean(y), np.mean(y_pred)\n",
    "        var_t, var_p = np.var(y, ddof=1), np.var(y_pred, ddof=1)\n",
    "        ccc = (2 * np.cov(y, y_pred)[0,1]) / (var_t + var_p + (mean_t - mean_p)**2)\n",
    "        \n",
    "        metrics = {\n",
    "            'concordance_correlation_coefficient': ccc,\n",
    "            'mean_squared_error': mean_squared_error(y, y_pred),\n",
    "            'r2_score': r2_score(y, y_pred)\n",
    "        }\n",
    "        \n",
    "        ## Calculate predictions for continuous targets\n",
    "        predictions_df = pd.DataFrame({\n",
    "            'Donor ID': donor_ids,\n",
    "            'true_value': y,\n",
    "            'predicted_value': y_pred\n",
    "        })\n",
    "    \n",
    "    ## Save predictions\n",
    "    predictions_df.to_csv(f'./output/{region}/{region}_{target}_train_predictions.csv', index=False)\n",
    "    \n",
    "    ## Save model\n",
    "    joblib.dump(model, f'./output/{region}/{region}_{target}_model.pkl')\n",
    "    \n",
    "    ## Save metrics\n",
    "    metrics_df = pd.DataFrame(list(metrics.items()), columns=['metric_name', 'metric_value'])\n",
    "    metrics_df.to_csv(f'./output/{region}/{region}_{target}_train_metrics.csv', index=False)\n",
    "    \n",
    "    ## Save feature importance\n",
    "    importance_df = pd.DataFrame({'feature': X.columns, 'importance': model.feature_importances_})\n",
    "    importance_df.sort_values('importance', ascending=False).to_csv(\n",
    "        f'./output/{region}/{region}_{target}_train_feature_importance.csv', index=False)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "26c19eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Process all targets per region with their respective hyperparameters\n",
    "\n",
    "hyperparams = load_hyperparams()\n",
    "\n",
    "for csv_file in os.listdir('data/'):\n",
    "    if csv_file in ['dataset_mtg_ordered.csv', 'dataset_a9_ordered.csv']:\n",
    "        \n",
    "        region = 'MTG' if 'mtg' in csv_file else 'A9'\n",
    "        df = pd.read_csv(f'data/{csv_file}')\n",
    "        \n",
    "        for target in all_target_columns:\n",
    "            train_final_model(df, target, region, hyperparams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
